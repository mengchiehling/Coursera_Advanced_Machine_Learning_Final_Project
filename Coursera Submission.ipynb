{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark - A_Wish Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date_block_num  shop_id  item_id    0     1    2    3    4    5    6    7  \\\n",
       "0                     0       30  0.0  31.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1                     0       31  0.0  11.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2                     0       32  6.0  10.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3                     0       33  3.0   3.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4                     0       35  1.0  14.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "date_block_num ...    24   25   26   27   28   29   30   31   32   33  \n",
       "0              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import GPyOpt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') # 214200 rows\n",
    "\n",
    "shops_df['city_id'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city_id'] = pd.Categorical(shops_df['city_id']).codes\n",
    "\n",
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "train_piv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.merge(train_piv, shops_df, how='left', on=['shop_id'])\n",
    "X_train = pd.merge(X_train, items_df, how='left', on=['item_id'])\n",
    "X_train = pd.merge(X_train, icats_df, how='left', on=['item_category_id'])\n",
    "Y_train = train_piv[33]\n",
    "\n",
    "X_train.drop(labels=['shop_id', 'item_id', 'shop_name', 'item_name', 'item_category_name', 33], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = pd.merge(test_df, train_piv, how='left', on=['shop_id', 'item_id']).fillna(0)\n",
    "X_test = pd.merge(X_test, shops_df, how='left', on=['shop_id'])\n",
    "X_test = pd.merge(X_test, items_df, how='left', on=['item_id'])\n",
    "X_test = pd.merge(X_test, icats_df, how='left', on=['item_category_id'])\n",
    "\n",
    "X_test.drop(labels=['shop_id', 'item_id', 'shop_name', 'item_name', 'item_category_name', 'ID', 0], axis=1, inplace=True)\n",
    "\n",
    "for i in range(33):\n",
    "    X_test.rename(columns={i+1: i}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, max_depth=10)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "preds = rf.predict(X_train)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline = -cross_val_score(rf, X_train, Y_train, scoring='mean_squared_error', cv=5, n_jobs=-1).mean()\n",
    "print baseline # 15.041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(parameters):\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=int(parameters[1]),\n",
    "                                              n_estimators=int(parameters[0]))\n",
    "    \n",
    "    score = -cross_val_score(rf, X_train, Y_train, scoring='mean_squared_error', cv=5, n_jobs=-1).mean()\n",
    "    score = np.array(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bounds (NOTE: define continuous variables first, then discrete!)\n",
    "bounds = [\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (10, 30)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (100, 1000)}\n",
    "             ]\n",
    "\n",
    "np.random.seed(777)\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,\n",
    "                                                initial_design_numdata=4,\n",
    "                                                model_type='sparseGP',\n",
    "                                                acquisition_type='MPI',\n",
    "                                                acquisition_par=0.1,\n",
    "                                                exact_eval=True)\n",
    "\n",
    "max_iter = 50\n",
    "\n",
    "optimizer.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', np.min(optimizer.Y), 'Gain:', baseline/np.min(optimizer.Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dict = {}\n",
    "\n",
    "for column, importance in zip(X_train.columns, rf.feature_importances_):\n",
    "    my_dict[column] = importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_red = X_train.copy()\n",
    "\n",
    "for column in X_train.columns:\n",
    "    if my_dict[column] <=0.005:\n",
    "        X_train_red.drop(labels=[column], axis=1, inplace=True)\n",
    "\n",
    "rf_red = RandomForestRegressor(n_estimators=400, max_depth=10)        \n",
    "        \n",
    "rf_red.fit(X_train_red, Y_train)\n",
    "\n",
    "preds = rf_red.predict(X_train_red)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918654453964\n",
      "1.3057289996\n",
      "CPU times: user 1.21 s, sys: 591 ms, total: 1.8 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(tree_method='gpu_hist')\n",
    "\n",
    "xgb.fit(X_train, Y_train)\n",
    "\n",
    "preds = xgb.predict(X_train)\n",
    "\n",
    "print(r2_score(Y_train, preds)) \n",
    "print(mean_squared_error(Y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4780459952\n",
      "CPU times: user 4.12 s, sys: 1.55 s, total: 5.66 s\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "baseline = -cross_val_score(xgb, X_train, Y_train, scoring='mean_squared_error', cv=5).mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(parameters):\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    xgb = XGBRegressor(learning_rate=parameters[0],\n",
    "                       max_depth=int(parameters[2]),\n",
    "                       n_estimators=int(parameters[3]),\n",
    "                       gamma=int(parameters[1]),\n",
    "                       min_child_weight = parameters[4],\n",
    "                       tree_method='gpu_hist')\n",
    "    \n",
    "    score = -cross_val_score(xgb, X_train, Y_train, scoring='mean_squared_error', cv=5).mean()\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "bounds = [\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.5)},\n",
    "            {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (3, 20)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 1000)},\n",
    "            {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}\n",
    "         ]\n",
    "\n",
    "np.random.seed(777)\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,\n",
    "                                                initial_design_numdata=4,\n",
    "                                                model_type='GP',\n",
    "                                                acquisition_type='MPI',\n",
    "                                                acquisition_par=0.1,\n",
    "                                                exact_eval=True)\n",
    "\n",
    "max_iter = 200\n",
    "\n",
    "optimizer.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('MSE:', np.min(optimizer.Y), 'Gain:', baseline/np.min(optimizer.Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') # 214200 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_shops = test_df.shop_id.unique()\n",
    "train_df = train_df[train_df.shop_id.isin(test_shops)]\n",
    "test_items = test_df.item_id.unique()\n",
    "train_df = train_df[train_df.item_id.isin(test_items)]\n",
    "\n",
    "print('train:', train_df.shape, 'test:', test_df.shape, 'items:', items_df.shape, 'shops:', shops_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_only = test_df[~test_df['item_id'].isin(train_df['item_id'].unique())]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group by\n",
    "train_grp = train_df.groupby(['date_block_num','shop_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# price mean by month\n",
    "train_price = pd.DataFrame(train_grp.mean()['item_price']).reset_index()\n",
    "train_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count summary by month\n",
    "train_monthly = pd.DataFrame(train_grp.sum()['item_cnt_day']).reset_index()\n",
    "train_monthly.rename(columns={'item_cnt_day':'item_cnt'}, inplace=True)\n",
    "train_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "train_piv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp = train_monthly.groupby(['shop_id', 'item_id'])\n",
    "train_shop = grp.agg({'item_cnt':['mean','median','std']}).reset_index()\n",
    "train_shop.columns = ['shop_id','item_id','cnt_mean_shop','cnt_med_shop','cnt_std_shop']\n",
    "train_shop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cat_monthly = pd.merge(train_monthly, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_monthly.groupby(['shop_id', 'item_category_id'])\n",
    "train_shop_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_shop_cat.columns = ['shop_id','item_category_id','cnt_mean_cat_shop']\n",
    "train_shop_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_last = train_monthly[train_monthly['date_block_num']==33]\n",
    "train_last = train_last.drop(['date_block_num'], axis=1).rename(columns={'item_cnt':'cnt_sum_last'})\n",
    "train_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prev month\n",
    "train_prev = train_monthly.copy()\n",
    "train_prev['date_block_num'] = train_prev['date_block_num'] + 1\n",
    "train_prev = train_prev.rename(columns={'item_cnt':'cnt_sum_prev'})\n",
    "train_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cat_prev = pd.merge(train_prev, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_prev.groupby(['date_block_num','shop_id','item_category_id'])\n",
    "train_cat_prev = grp['cnt_sum_prev'].sum().reset_index()\n",
    "train_cat_prev = train_cat_prev.rename(columns={'cnt_sum_prev':'cnt_sum_cat_prev'})\n",
    "train_cat_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = np.arange(34)\n",
    "pivT = train_piv[col].T\n",
    "evm_s = pivT.ewm(span=12).mean().T\n",
    "evm_l = pivT.ewm(span=26).mean().T\n",
    "\n",
    "macd = evm_s - evm_l\n",
    "sig = macd.ewm(span=9).mean()\n",
    "\n",
    "train_piv_key = train_piv.loc[:,['shop_id','item_id']]\n",
    "train_evm_list = []\n",
    "\n",
    "for c in col:\n",
    "    sub_evm_s = pd.DataFrame(evm_s.loc[:,c]).rename(columns={c:'cnt_evm_s_prev'})\n",
    "    sub_evm_l = pd.DataFrame(evm_l.loc[:,c]).rename(columns={c:'cnt_evm_l_prev'})\n",
    "    sub_macd = pd.DataFrame(macd.loc[:,c]).rename(columns={c:'cnt_macd_prev'})\n",
    "    sub_sig = pd.DataFrame(sig.loc[:,c]).rename(columns={c:'cnt_sig_prev'})\n",
    "    \n",
    "    sub_evm = pd.concat([train_piv_key, sub_evm_s, sub_evm_l, sub_macd, sub_sig], axis=1)\n",
    "    sub_evm['date_block_num'] = c + 1\n",
    "    train_evm_list.append(sub_evm)\n",
    "    \n",
    "train_evm_prev = pd.concat(train_evm_list)\n",
    "#train_evm_prev.head()\n",
    "train_evm_prev.query(\"shop_id == 2 & item_id == 30\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "item_cats = pd.merge(icats_df, pd.get_dummies(icats_df['item_category_group'], prefix='item_category_group', drop_first=True), left_index=True, right_index=True)\n",
    "item_cats.drop(['item_category_group'], axis=1, inplace=True)\n",
    "\n",
    "shops_df['city'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city'] = pd.Categorical(shops_df['city']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeFeature(source): \n",
    "    d = source\n",
    "    d = pd.merge(d, items_df, on=['item_id'], how='left')\n",
    "    d = pd.merge(d, item_cats, on=['item_category_id'], how='left')\n",
    "    d = pd.merge(d, shops_df, on=['shop_id'], how='left')\n",
    "\n",
    "    d = pd.merge(d, train_price, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop, on=['shop_id','item_id'], how='left')\n",
    "    #d = pd.merge(d, train_shop_cat, on=['shop_id','item_category_id'], how='left')\n",
    "    #d = pd.merge(d, train_last, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_evm_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_cat_prev, on=['date_block_num','shop_id','item_category_id'], how='left')\n",
    "\n",
    "    d.drop(['shop_id','shop_name','item_id','item_name','item_category_id','item_category_name'], axis=1, inplace=True)\n",
    "    d.fillna(0.0, inplace=True)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = mergeFeature(train_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['date_block_num'] = 34\n",
    "\n",
    "X_test = mergeFeature(test_df.drop(['ID'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_train = train_set.drop(['item_cnt'], axis=1)\n",
    "Y_train = train_set['item_cnt']\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=25, max_depth=12, learning_rate=0.1, subsample=1, colsample_bytree=1, eval_metric='rmse')\n",
    "\n",
    "xgb.fit(X_train, Y_train)\n",
    "\n",
    "preds = xgb.predict(X_train)\n",
    "\n",
    "print r2_score(Y_train, preds) #0.955330774186\n",
    "print mean_squared_error(Y_train, preds) #5.69332522603"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') \n",
    "\n",
    "test_shops = test_df.shop_id.unique()\n",
    "train_df = train_df[train_df.shop_id.isin(test_shops)]\n",
    "test_items = test_df.item_id.unique()\n",
    "train_df = train_df[train_df.item_id.isin(test_items)]\n",
    "\n",
    "print('train:', train_df.shape, 'test:', test_df.shape, 'items:', items_df.shape, 'shops:', shops_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_grp = train_df.groupby(['date_block_num','shop_id','item_id'])\n",
    "\n",
    "train_price = pd.DataFrame(train_grp.mean()['item_price']).reset_index()\n",
    "\n",
    "test_grp = train_df.groupby(['shop_id', 'item_id'])\n",
    "\n",
    "test_price = pd.DataFrame(test_grp.mean()['item_price']).reset_index()\n",
    "\n",
    "test_df['date_block_num'] = 34\n",
    "test_df['season'] = np.sin(34 * np.pi * 2/12.0)\n",
    "\n",
    "a = test_df[['date_block_num', 'shop_id', 'item_id']]\n",
    "a = pd.merge(a, test_price, how='left', on=['shop_id', 'item_id'])\n",
    "train_price = pd.concat((train_price, a), ignore_index=True)\n",
    "\n",
    "train_monthly = pd.DataFrame(train_grp.sum()['item_cnt_day']).reset_index()\n",
    "\n",
    "train_monthly.rename(columns={'item_cnt_day':'item_cnt'}, inplace=True)\n",
    "\n",
    "train_monthly['season'] = np.sin(train_monthly['date_block_num']*2*np.pi/12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "\n",
    "grp = train_monthly.groupby(['shop_id', 'item_id'])\n",
    "train_shop = grp.agg({'item_cnt':['mean','median','std']}).reset_index()\n",
    "train_shop.columns = ['shop_id','item_id','cnt_mean_shop','cnt_med_shop','cnt_std_shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cat_monthly = pd.merge(train_monthly, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_monthly.groupby(['shop_id', 'item_category_id'])\n",
    "train_shop_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_shop_cat.columns = ['shop_id','item_category_id','cnt_mean_cat_shop']\n",
    "train_shop_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shops_df['city_id'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city_id'] = pd.Categorical(shops_df['city_id']).codes\n",
    "shops_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct cnt_mean_cat_shop\n",
    "\n",
    "train_cat_shop_monthly = pd.merge(train_cat_monthly, shops_df, on=['shop_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp = train_cat_shop_monthly.groupby(['city_id', 'item_category_id'])\n",
    "train_city_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_city_cat.columns = ['city_id', 'item_category_id', 'cnt_mean_cat_city']\n",
    "train_city_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_last = train_monthly[train_monthly['date_block_num']==33]\n",
    "train_last = train_last.drop(['date_block_num'], axis=1).rename(columns={'item_cnt':'cnt_sum_last'})\n",
    "train_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prev month\n",
    "train_prev = train_monthly.copy().drop(['season'], axis=1)\n",
    "train_prev['date_block_num'] = train_prev['date_block_num'] + 1\n",
    "train_prev = train_prev.rename(columns={'item_cnt':'cnt_sum_prev'})\n",
    "\n",
    "train_cat_prev = pd.merge(train_prev, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_prev.groupby(['date_block_num','shop_id','item_category_id'])\n",
    "train_cat_prev = grp['cnt_sum_prev'].sum().reset_index()\n",
    "train_cat_prev = train_cat_prev.rename(columns={'cnt_sum_prev':'cnt_sum_cat_prev'})\n",
    "train_cat_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = np.arange(34)\n",
    "pivT = train_piv[col].T\n",
    "evm_s = pivT.ewm(span=12).mean().T\n",
    "evm_l = pivT.ewm(span=26).mean().T\n",
    "\n",
    "macd = evm_s - evm_l\n",
    "sig = macd.ewm(span=9).mean()\n",
    "\n",
    "train_piv_key = train_piv.loc[:,['shop_id','item_id']]\n",
    "train_evm_list = []\n",
    "\n",
    "for c in col:\n",
    "    sub_evm_s = pd.DataFrame(evm_s.loc[:,c]).rename(columns={c:'cnt_evm_s_prev'})\n",
    "    sub_evm_l = pd.DataFrame(evm_l.loc[:,c]).rename(columns={c:'cnt_evm_l_prev'})\n",
    "    sub_macd = pd.DataFrame(macd.loc[:,c]).rename(columns={c:'cnt_macd_prev'})\n",
    "    sub_sig = pd.DataFrame(sig.loc[:,c]).rename(columns={c:'cnt_sig_prev'})\n",
    "    \n",
    "    sub_evm = pd.concat([train_piv_key, sub_evm_s, sub_evm_l, sub_macd, sub_sig], axis=1)\n",
    "    sub_evm['date_block_num'] = c + 1\n",
    "    train_evm_list.append(sub_evm)\n",
    "    \n",
    "train_evm_prev = pd.concat(train_evm_list)\n",
    "#train_evm_prev.head()\n",
    "train_evm_prev.query(\"shop_id == 2 & item_id == 30\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cat_monthly.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "train_cat_group_monthly = pd.merge(train_cat_monthly, icats_df, on=['item_category_id'], how='left')\n",
    "\n",
    "grp = train_cat_group_monthly.groupby(['item_category_group'])\n",
    "train_group = grp.agg({'item_cnt':['mean', 'median', 'std']}).reset_index()\n",
    "train_group.columns = ['item_category_group', 'cnt_mean_group', 'cnt_median_group', 'cnt_std_group']\n",
    "train_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeFeature(source): \n",
    "    d = source\n",
    "    d = pd.merge(d, items_df, on=['item_id'], how='left')\n",
    "    d = pd.merge(d, icats_df, on=['item_category_id'], how='left')\n",
    "    d = pd.merge(d, shops_df, on=['shop_id'], how='left')\n",
    "\n",
    "    d = pd.merge(d, train_price, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop_cat, on=['shop_id','item_category_id'], how='left')\n",
    "    d = pd.merge(d, train_city_cat, on=['city_id','item_category_id'], how='left')\n",
    "    d = pd.merge(d, train_group, on=['item_category_group'], how='left')\n",
    "    #d = pd.merge(d, train_last, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_evm_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_cat_prev, on=['date_block_num','shop_id','item_category_id'], how='left')\n",
    "\n",
    "    d.drop(['date_block_num', 'shop_id','shop_name','item_id','item_name','item_category_id','item_category_name', 'item_category_group', 'city_id'], axis=1, inplace=True)\n",
    "    d.fillna(0.0, inplace=True)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_rescale(train, test):\n",
    "    \n",
    "    d = pd.concat([train, test], ignore_index=True)\n",
    "    \n",
    "    d['item_price_inv'] = d['item_price'].values.min()/d['item_price']\n",
    "    d.drop(['item_price'], axis=1, inplace=True)\n",
    "    d['item_price_inv'].fillna(0, inplace=True)\n",
    "    \n",
    "    columns = ['cnt_mean_shop', 'cnt_std_shop', 'cnt_mean_cat_shop', 'cnt_mean_cat_city', 'cnt_mean_group',\n",
    "               'cnt_median_group', 'cnt_std_group', 'cnt_sum_prev', 'cnt_evm_s_prev', 'cnt_evm_l_prev',\n",
    "               'cnt_macd_prev', 'cnt_sig_prev', 'cnt_sum_cat_prev']\n",
    "    \n",
    "    for column in columns:\n",
    "    \n",
    "        d[column] = d[column]/d[column].values.std()\n",
    "    \n",
    "    return d.iloc[:len(train)], d.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = mergeFeature(train_monthly)\n",
    "\n",
    "X_train = train_set.drop(['item_cnt'], axis=1)\n",
    "Y_train = train_set[['item_cnt']]\n",
    "\n",
    "X_test = mergeFeature(test_df.drop(['ID'], axis=1))\n",
    "\n",
    "X_train, X_test = num_rescale(X_train, X_test)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train.to_csv(\"X_train.csv.gz\", index=False, compression='gzip')\n",
    "X_test['ID'] = test_df['ID']\n",
    "X_test.to_csv(\"X_test.csv.gz\", index=False, compression='gzip')\n",
    "Y_train.to_csv(\"Y_train.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the X_train, Y_train, X_test are produced, we can work from this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
