{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark - A_Wish Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>date_block_num</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "date_block_num  shop_id  item_id    0     1    2    3    4    5    6    7  \\\n",
       "0                     0       30  0.0  31.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1                     0       31  0.0  11.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2                     0       32  6.0  10.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3                     0       33  3.0   3.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4                     0       35  1.0  14.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "date_block_num ...    24   25   26   27   28   29   30   31   32   33  \n",
       "0              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4              ...   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm_notebook\n",
    "# import GPyOpt\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') # 214200 rows\n",
    "\n",
    "shops_df['city_id'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city_id'] = pd.Categorical(shops_df['city_id']).codes\n",
    "\n",
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "train_piv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.merge(train_piv, shops_df, how='left', on=['shop_id'])\n",
    "X_train = pd.merge(X_train, items_df, how='left', on=['item_id'])\n",
    "X_train = pd.merge(X_train, icats_df, how='left', on=['item_category_id'])\n",
    "Y_train = train_piv[33]\n",
    "\n",
    "X_train.drop(labels=['shop_name', 'item_name', 'item_category_name', 33], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.merge(test_df, train_piv, how='left', on=['shop_id', 'item_id']).fillna(0)\n",
    "X_test = pd.merge(X_test, shops_df, how='left', on=['shop_id'])\n",
    "X_test = pd.merge(X_test, items_df, how='left', on=['item_id'])\n",
    "X_test = pd.merge(X_test, icats_df, how='left', on=['item_category_id'])\n",
    "\n",
    "X_test.drop(labels=['shop_name', 'item_name', 'item_category_name', 'ID', 0], axis=1, inplace=True)\n",
    "\n",
    "for i in range(33):\n",
    "    X_test.rename(columns={i+1: i}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Clustering on city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NearestNeighborsFeats(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "        This class should implement KNN features extraction \n",
    "    '''\n",
    "    def __init__(self, n_jobs, k_list, metric, month_history, n_classes=None, n_neighbors=None, eps=0.000001):\n",
    "        self.n_jobs = n_jobs\n",
    "        self.k_list = k_list\n",
    "        self.metric = metric\n",
    "        self.month_history = month_history\n",
    "        \n",
    "        if n_neighbors is None:\n",
    "            self.n_neighbors = max(k_list) \n",
    "        else:\n",
    "            self.n_neighbors = n_neighbors\n",
    "            \n",
    "        self.eps = eps        \n",
    "        self.n_classes_ = n_classes\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Set's up the train set and self.NN object\n",
    "        '''\n",
    "        \n",
    "        self.X = X\n",
    "        \n",
    "        # Create a NearestNeighbors (NN) object. We will use it in `predict` function \n",
    "        self.NN = NearestNeighbors(n_neighbors=max(self.k_list), \n",
    "                                   metric=self.metric, \n",
    "                                   n_jobs=1, \n",
    "                                   algorithm='auto')\n",
    "        self.NN.fit(X)\n",
    "        \n",
    "        # Store labels \n",
    "        self.y_train = y\n",
    "        \n",
    "        # Save how many classes we have\n",
    "        self.n_classes = np.unique(y).shape[0] if self.n_classes_ is None else self.n_classes_\n",
    "        \n",
    "        \n",
    "    def predict(self, X):       \n",
    "        '''\n",
    "            Produces KNN features for every object of a dataset X\n",
    "        '''\n",
    "        if self.n_jobs == 1:\n",
    "            test_feats = []\n",
    "            for i in tqdm_notebook(range(X.shape[0])):\n",
    "                test_feats.append(self.get_features_for_one(X[i:i+1]))\n",
    "        else:\n",
    "            '''\n",
    "                 *Make it parallel*\n",
    "                     Number of threads should be controlled by `self.n_jobs`  \n",
    "                     \n",
    "                     \n",
    "                     You can use whatever you want to do it\n",
    "                     For Python 3 the simplest option would be to use \n",
    "                     `multiprocessing.Pool` (but don't use `multiprocessing.dummy.Pool` here)\n",
    "                     You may try use `joblib` but you will most likely encounter an error, \n",
    "                     that you will need to google up (and eventually it will work slowly)\n",
    "                     \n",
    "                     For Python 2 I also suggest using `multiprocessing.Pool` \n",
    "                     You will need to use a hint from this blog \n",
    "                     http://qingkaikong.blogspot.ru/2016/12/python-parallel-method-in-class.html\n",
    "                     I could not get `joblib` working at all for this code \n",
    "                     (but in general `joblib` is very convenient)\n",
    "                     \n",
    "            '''\n",
    "            test_feats = []\n",
    "            \n",
    "            pool = Pool(processes=self.n_jobs)\n",
    "            \n",
    "            for i in range(X.shape[0]):\n",
    "                test_feats.append(pool.apply_async(self.get_features_for_one, (X[i:i+1],)))\n",
    "            \n",
    "            pool.close()\n",
    "            \n",
    "            test_feats = [res.get() for res in test_feats]\n",
    "            pool.join()\n",
    "            \n",
    "            \n",
    "#             assert False, 'You need to implement it for n_jobs > 1'\n",
    "            \n",
    "        return np.vstack(test_feats)\n",
    "        \n",
    "        \n",
    "    def get_features_for_one(self, x):\n",
    "        '''\n",
    "            Computes KNN features for a single object `x`\n",
    "        '''\n",
    "\n",
    "        NN_output = self.NN.kneighbors(x) # return dist: array representing the lengths to points\n",
    "                                          #        ind: indices of the nearest points in the population index\n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores indices of the neighbors\n",
    "        neighs = NN_output[1][0] # cast down to 1-d array\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores distances to corresponding neighbors\n",
    "        neighs_dist = NN_output[0][0] # cast down to 1-d array\n",
    "        \n",
    "        # Vector of size `n_neighbors`\n",
    "        # Stores labels of corresponding neighbors\n",
    "        neighs_y = self.y_train[neighs] \n",
    "\n",
    "#         return_list = []\n",
    "        \n",
    "        for k in self.k_list:\n",
    "            mean = self.X[neighs[:k]].mean(axis=0)\n",
    "            std = self.X[neighs[:k]].std(axis=0)\n",
    "            \n",
    "        return_list = [mean[-self.month_history:], std[-self.month_history:]]\n",
    "        \n",
    "        knn_feats = np.hstack(return_list)\n",
    "        \n",
    "        return knn_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def KNN_feature_transformation(X_train, X_test, categorical_features, metrics):\n",
    "    \n",
    "    x_train = X_train.drop(labels=categorical_features, axis=1).values\n",
    "    x_test = X_test.drop(labels=categorical_features, axis=1).values\n",
    "    \n",
    "    k_list = [10, 20, 30]\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    \n",
    "    print \"5-folds training on the x_train\"\n",
    "    \n",
    "    for metric in metrics:\n",
    "        for train_index, test_index in tqdm_notebook(kf.split(x_train)):\n",
    "    \n",
    "            # Create instance of our KNN feature extractor\n",
    "            NNF = NearestNeighborsFeats(n_jobs=1, k_list=k_list, metric=metric, month_history=6)\n",
    "\n",
    "            train_kf = x_train[train_index]\n",
    "\n",
    "            # Fit on train_kf set\n",
    "            NNF.fit(train_kf, X_train['city_id'].values[train_index])\n",
    "\n",
    "            # Get features for \"train\" with KFold regularization\n",
    "            test_kf = x_train[test_index]\n",
    "            test_kf_knn = NNF.predict(test_kf)\n",
    "\n",
    "            test_kf_knn_cat = X_train[categorical_features].values[test_index]\n",
    "\n",
    "            data = np.hstack((test_kf_knn_cat, test_kf_knn))\n",
    "\n",
    "            columns = categorical_features + [\"feature_{:03d}\".format(i) for i in range(test_kf_knn.shape[1])]\n",
    "\n",
    "            df_0 = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "            # concat the result of each fold\n",
    "\n",
    "            df_train = pd.concat([df_train, df_0], ignore_index=True)\n",
    "            \n",
    "    NNF.fit(x_train, x_train['city_id'].values)\n",
    "    test_knn = NNF.predict(x_test)\n",
    "    \n",
    "    test_cat = X_test[categorical_features].values\n",
    "\n",
    "    data = np.hstack((test_cat, test_knn))\n",
    "\n",
    "    columns = categorical_features + [\"feature_{:03d}\".format(i) for i in range(test_knn.shape[1])]\n",
    "    \n",
    "    df_test = pd.DataFrame(data=data, columns=columns)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-folds training on the x_train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c63baf45958401f8597c40310de759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style=u'info', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aad5f9eafe54398a23a1b8d84189fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84825), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a593f4cd3eaa46c9863c0e8469c38819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84825), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c38d595ee84a57ae69b657153c62d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84825), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5174178901814848af28d6efc0ffc5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84825), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a4506a96f44212b378510a18cfabc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84824), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mling/.local/lib/python2.7/site-packages/ipykernel_launcher.py:43: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-71bbf9ae6da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcategorical_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'shop_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'city_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_category_group'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train_city\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_city\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNN_feature_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-564960a4cffd>\u001b[0m in \u001b[0;36mKNN_feature_transformation\u001b[0;34m(X_train, X_test, categorical_features, metrics)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mNNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'city_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mtest_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "metrics = ['cosine']\n",
    "categorical_features = ['shop_id', 'item_id', 'city_id', 'item_category_group']\n",
    "\n",
    "X_train_city, X_test_city = KNN_feature_transformation(X_train, X_test, categorical_features, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and standard deviation of the last six month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_6_month_mean = X_train[[27,28,29,30,31,32]].mean(axis=1)\n",
    "prev_6_month_std = X_train[[27,28,29,30,31,32]].std(axis=1)\n",
    "\n",
    "X_train['prev_6_month_mean'] = prev_6_month_mean\n",
    "X_train['prev_6_month_std'] = prev_6_month_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_from_previous_month = X_train[[ix for ix in range(1, 33)]]\n",
    "change_from_previous_month.columns = [ix for ix in range(0, 32)]\n",
    "\n",
    "change_from_previous_month = change_from_previous_month - X_train[[ix for ix in range(0, 32)]]\n",
    "change_from_previous_month.columns = [\"change_{}\".format(ix) for ix in range(0, 32)]\n",
    "\n",
    "X_train['change_prev_6_month_mean'] = change_from_previous_month[[\"change_{}\".format(ix) for ix in range(26, 32)]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=400, max_depth=10, random_state=42)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "preds = rf.predict(X_train)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "X_new = SelectKBest(mutual_info_classif, k=12).fit_transform(X_train, Y_train)\n",
    "\n",
    "rf.fit(X_new, Y_train)\n",
    "\n",
    "preds = rf.predict(X_new)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = []\n",
    "\n",
    "for column, importance in zip(X_train.columns, rf.feature_importances_):\n",
    "    feature_importance.append((column, importance))\n",
    "    \n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summation = 0\n",
    "\n",
    "for _, a in feature_importance:\n",
    "    if a > 0.001:\n",
    "        summation +=a\n",
    "        \n",
    "print summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline = -cross_val_score(rf, X_train, Y_train, scoring='mean_squared_error', cv=5, n_jobs=-1).mean()\n",
    "print baseline # 15.041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(parameters):\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=int(parameters[1]),\n",
    "                                              n_estimators=int(parameters[0]))\n",
    "    \n",
    "    score = -cross_val_score(rf, X_train, Y_train, scoring='mean_squared_error', cv=5, n_jobs=-1).mean()\n",
    "    score = np.array(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds (NOTE: define continuous variables first, then discrete!)\n",
    "bounds = [\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (10, 30)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (100, 1000)}\n",
    "             ]\n",
    "\n",
    "np.random.seed(777)\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,\n",
    "                                                initial_design_numdata=4,\n",
    "                                                model_type='sparseGP',\n",
    "                                                acquisition_type='MPI',\n",
    "                                                acquisition_par=0.1,\n",
    "                                                exact_eval=True)\n",
    "\n",
    "max_iter = 50\n",
    "\n",
    "optimizer.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', np.min(optimizer.Y), 'Gain:', baseline/np.min(optimizer.Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_red = X_train.copy()\n",
    "\n",
    "for column in X_train.columns:\n",
    "    if my_dict[column] <=0.005:\n",
    "        X_train_red.drop(labels=[column], axis=1, inplace=True)\n",
    "\n",
    "rf_red = RandomForestRegressor(n_estimators=400, max_depth=10)        \n",
    "        \n",
    "rf_red.fit(X_train_red, Y_train)\n",
    "\n",
    "preds = rf_red.predict(X_train_red)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb = XGBRegressor(tree_method='gpu_hist')\n",
    "\n",
    "xgb.fit(X_train, Y_train)\n",
    "\n",
    "preds = xgb.predict(X_train)\n",
    "\n",
    "print(r2_score(Y_train, preds)) \n",
    "print(mean_squared_error(Y_train, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "baseline = -cross_val_score(xgb, X_train, Y_train, scoring='mean_squared_error', cv=5).mean()\n",
    "print(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(parameters):\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    xgb = XGBRegressor(learning_rate=parameters[0],\n",
    "                       max_depth=int(parameters[2]),\n",
    "                       n_estimators=int(parameters[3]),\n",
    "                       gamma=int(parameters[1]),\n",
    "                       min_child_weight = parameters[4],\n",
    "                       tree_method='gpu_hist')\n",
    "    \n",
    "    score = -cross_val_score(xgb, X_train, Y_train, scoring='mean_squared_error', cv=5).mean()\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "bounds = [\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.5)},\n",
    "            {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (3, 20)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (1, 1000)},\n",
    "            {'name': 'min_child_weight', 'type': 'discrete', 'domain': (1, 10)}\n",
    "         ]\n",
    "\n",
    "np.random.seed(777)\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,\n",
    "                                                initial_design_numdata=4,\n",
    "                                                model_type='GP',\n",
    "                                                acquisition_type='MPI',\n",
    "                                                acquisition_par=0.1,\n",
    "                                                exact_eval=True)\n",
    "\n",
    "max_iter = 200\n",
    "\n",
    "optimizer.run_optimization(max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', np.min(optimizer.Y), 'Gain:', baseline/np.min(optimizer.Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb = LGBMRegressor(n_estimators=400, max_depth=10)\n",
    "\n",
    "lgb.fit(X_train, Y_train)\n",
    "\n",
    "preds = lgb.predict(X_train)\n",
    "\n",
    "print r2_score(Y_train, preds) \n",
    "print mean_squared_error(Y_train, preds) \n",
    "\n",
    "baseline = -cross_val_score(lgb, X_train, Y_train, scoring='mean_squared_error', cv=5, n_jobs=-1).mean()\n",
    "print baseline # 15.041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(parameters):\n",
    "    parameters = parameters[0]\n",
    "    \n",
    "    lgb = LGBMRegressor(learning_rate=parameters[0],\n",
    "                        max_depth=int(parameters[1]),\n",
    "                        n_estimators=int(parameters[2]),\n",
    "                        num_leaves=int(parameters[3]),\n",
    "                        min_data_in_leaf = int(parameters[4]))\n",
    "    \n",
    "    score = -cross_val_score(lgb, X_train, Y_train, scoring='mean_squared_error', cv=5).mean()\n",
    "    score = np.array(score)\n",
    "    return score\n",
    "\n",
    "bounds = [\n",
    "            {'name': 'learning_rate', 'type': 'continuous', 'domain': (0.001, 0.1)},\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': (3, 20)},\n",
    "            {'name': 'n_estimators', 'type': 'discrete', 'domain': (100, 1000)},\n",
    "            {'name': 'num_leaves', 'type': 'discrete', 'domain': (10, 50)},\n",
    "            {'name': 'min_data_in_leaf', 'type': 'discrete', 'domain': (20, 50)}\n",
    "         ]\n",
    "\n",
    "np.random.seed(777)\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(f=f, domain=bounds,\n",
    "                                                initial_design_numdata=4,\n",
    "                                                model_type='GP',\n",
    "                                                acquisition_type='MPI',\n",
    "                                                acquisition_par=0.1,\n",
    "                                                exact_eval=True)\n",
    "\n",
    "max_iter = 200\n",
    "\n",
    "optimizer.run_optimization(max_iter)\n",
    "\n",
    "print('MSE:', np.min(optimizer.Y), 'Gain:', baseline/np.min(optimizer.Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') # 214200 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shops = test_df.shop_id.unique()\n",
    "train_df = train_df[train_df.shop_id.isin(test_shops)]\n",
    "test_items = test_df.item_id.unique()\n",
    "train_df = train_df[train_df.item_id.isin(test_items)]\n",
    "\n",
    "print('train:', train_df.shape, 'test:', test_df.shape, 'items:', items_df.shape, 'shops:', shops_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_only = test_df[~test_df['item_id'].isin(train_df['item_id'].unique())]['item_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by\n",
    "train_grp = train_df.groupby(['date_block_num','shop_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price mean by month\n",
    "train_price = pd.DataFrame(train_grp.mean()['item_price']).reset_index()\n",
    "train_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count summary by month\n",
    "train_monthly = pd.DataFrame(train_grp.sum()['item_cnt_day']).reset_index()\n",
    "train_monthly.rename(columns={'item_cnt_day':'item_cnt'}, inplace=True)\n",
    "train_monthly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "train_piv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = train_monthly.groupby(['shop_id', 'item_id'])\n",
    "train_shop = grp.agg({'item_cnt':['mean','median','std']}).reset_index()\n",
    "train_shop.columns = ['shop_id','item_id','cnt_mean_shop','cnt_med_shop','cnt_std_shop']\n",
    "train_shop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_monthly = pd.merge(train_monthly, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_monthly.groupby(['shop_id', 'item_category_id'])\n",
    "train_shop_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_shop_cat.columns = ['shop_id','item_category_id','cnt_mean_cat_shop']\n",
    "train_shop_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_last = train_monthly[train_monthly['date_block_num']==33]\n",
    "train_last = train_last.drop(['date_block_num'], axis=1).rename(columns={'item_cnt':'cnt_sum_last'})\n",
    "train_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prev month\n",
    "train_prev = train_monthly.copy()\n",
    "train_prev['date_block_num'] = train_prev['date_block_num'] + 1\n",
    "train_prev = train_prev.rename(columns={'item_cnt':'cnt_sum_prev'})\n",
    "train_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_prev = pd.merge(train_prev, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_prev.groupby(['date_block_num','shop_id','item_category_id'])\n",
    "train_cat_prev = grp['cnt_sum_prev'].sum().reset_index()\n",
    "train_cat_prev = train_cat_prev.rename(columns={'cnt_sum_prev':'cnt_sum_cat_prev'})\n",
    "train_cat_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.arange(34)\n",
    "pivT = train_piv[col].T\n",
    "evm_s = pivT.ewm(span=12).mean().T\n",
    "evm_l = pivT.ewm(span=26).mean().T\n",
    "\n",
    "macd = evm_s - evm_l\n",
    "sig = macd.ewm(span=9).mean()\n",
    "\n",
    "train_piv_key = train_piv.loc[:,['shop_id','item_id']]\n",
    "train_evm_list = []\n",
    "\n",
    "for c in col:\n",
    "    sub_evm_s = pd.DataFrame(evm_s.loc[:,c]).rename(columns={c:'cnt_evm_s_prev'})\n",
    "    sub_evm_l = pd.DataFrame(evm_l.loc[:,c]).rename(columns={c:'cnt_evm_l_prev'})\n",
    "    sub_macd = pd.DataFrame(macd.loc[:,c]).rename(columns={c:'cnt_macd_prev'})\n",
    "    sub_sig = pd.DataFrame(sig.loc[:,c]).rename(columns={c:'cnt_sig_prev'})\n",
    "    \n",
    "    sub_evm = pd.concat([train_piv_key, sub_evm_s, sub_evm_l, sub_macd, sub_sig], axis=1)\n",
    "    sub_evm['date_block_num'] = c + 1\n",
    "    train_evm_list.append(sub_evm)\n",
    "    \n",
    "train_evm_prev = pd.concat(train_evm_list)\n",
    "#train_evm_prev.head()\n",
    "train_evm_prev.query(\"shop_id == 2 & item_id == 30\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "item_cats = pd.merge(icats_df, pd.get_dummies(icats_df['item_category_group'], prefix='item_category_group', drop_first=True), left_index=True, right_index=True)\n",
    "item_cats.drop(['item_category_group'], axis=1, inplace=True)\n",
    "\n",
    "shops_df['city'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city'] = pd.Categorical(shops_df['city']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeature(source): \n",
    "    d = source\n",
    "    d = pd.merge(d, items_df, on=['item_id'], how='left')\n",
    "    d = pd.merge(d, item_cats, on=['item_category_id'], how='left')\n",
    "    d = pd.merge(d, shops_df, on=['shop_id'], how='left')\n",
    "\n",
    "    d = pd.merge(d, train_price, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop, on=['shop_id','item_id'], how='left')\n",
    "    #d = pd.merge(d, train_shop_cat, on=['shop_id','item_category_id'], how='left')\n",
    "    #d = pd.merge(d, train_last, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_evm_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_cat_prev, on=['date_block_num','shop_id','item_category_id'], how='left')\n",
    "\n",
    "    d.drop(['shop_id','shop_name','item_id','item_name','item_category_id','item_category_name'], axis=1, inplace=True)\n",
    "    d.fillna(0.0, inplace=True)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mergeFeature(train_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['date_block_num'] = 34\n",
    "\n",
    "X_test = mergeFeature(test_df.drop(['ID'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_train = train_set.drop(['item_cnt'], axis=1)\n",
    "Y_train = train_set['item_cnt']\n",
    "\n",
    "xgb = XGBRegressor(n_estimators=25, max_depth=12, learning_rate=0.1, subsample=1, colsample_bytree=1, eval_metric='rmse')\n",
    "\n",
    "xgb.fit(X_train, Y_train)\n",
    "\n",
    "preds = xgb.predict(X_train)\n",
    "\n",
    "print r2_score(Y_train, preds) #0.955330774186\n",
    "print mean_squared_error(Y_train, preds) #5.69332522603"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "items_df = pd.read_csv('Data/items.csv')\n",
    "shops_df = pd.read_csv('Data/shops.csv')\n",
    "icats_df = pd.read_csv(\"Data/item_categories.csv\")\n",
    "train_df = pd.read_csv(\"Data/sales_train.csv.gz\")\n",
    "test_df  = pd.read_csv('Data/test.csv.gz') \n",
    "\n",
    "test_shops = test_df.shop_id.unique()\n",
    "train_df = train_df[train_df.shop_id.isin(test_shops)]\n",
    "test_items = test_df.item_id.unique()\n",
    "train_df = train_df[train_df.item_id.isin(test_items)]\n",
    "\n",
    "print('train:', train_df.shape, 'test:', test_df.shape, 'items:', items_df.shape, 'shops:', shops_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grp = train_df.groupby(['date_block_num','shop_id','item_id'])\n",
    "\n",
    "train_price = pd.DataFrame(train_grp.mean()['item_price']).reset_index()\n",
    "\n",
    "test_grp = train_df.groupby(['shop_id', 'item_id'])\n",
    "\n",
    "test_price = pd.DataFrame(test_grp.mean()['item_price']).reset_index()\n",
    "\n",
    "test_df['date_block_num'] = 34\n",
    "test_df['season'] = np.sin(34 * np.pi * 2/12.0)\n",
    "\n",
    "a = test_df[['date_block_num', 'shop_id', 'item_id']]\n",
    "a = pd.merge(a, test_price, how='left', on=['shop_id', 'item_id'])\n",
    "train_price = pd.concat((train_price, a), ignore_index=True)\n",
    "\n",
    "train_monthly = pd.DataFrame(train_grp.sum()['item_cnt_day']).reset_index()\n",
    "\n",
    "train_monthly.rename(columns={'item_cnt_day':'item_cnt'}, inplace=True)\n",
    "\n",
    "train_monthly['season'] = np.sin(train_monthly['date_block_num']*2*np.pi/12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_piv = train_df.pivot_table(index=['shop_id','item_id'], columns='date_block_num', values='item_cnt_day',aggfunc='sum').fillna(0.0)    \n",
    "train_piv = train_piv.reset_index()\n",
    "\n",
    "grp = train_monthly.groupby(['shop_id', 'item_id'])\n",
    "train_shop = grp.agg({'item_cnt':['mean','median','std']}).reset_index()\n",
    "train_shop.columns = ['shop_id','item_id','cnt_mean_shop','cnt_med_shop','cnt_std_shop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_monthly = pd.merge(train_monthly, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_monthly.groupby(['shop_id', 'item_category_id'])\n",
    "train_shop_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_shop_cat.columns = ['shop_id','item_category_id','cnt_mean_cat_shop']\n",
    "train_shop_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops_df['city_id'] = shops_df.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\n",
    "shops_df['city_id'] = pd.Categorical(shops_df['city_id']).codes\n",
    "shops_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct cnt_mean_cat_shop\n",
    "\n",
    "train_cat_shop_monthly = pd.merge(train_cat_monthly, shops_df, on=['shop_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = train_cat_shop_monthly.groupby(['city_id', 'item_category_id'])\n",
    "train_city_cat = grp.agg({'item_cnt':['mean']}).reset_index()\n",
    "train_city_cat.columns = ['city_id', 'item_category_id', 'cnt_mean_cat_city']\n",
    "train_city_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_last = train_monthly[train_monthly['date_block_num']==33]\n",
    "train_last = train_last.drop(['date_block_num'], axis=1).rename(columns={'item_cnt':'cnt_sum_last'})\n",
    "train_last.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prev month\n",
    "train_prev = train_monthly.copy().drop(['season'], axis=1)\n",
    "train_prev['date_block_num'] = train_prev['date_block_num'] + 1\n",
    "train_prev = train_prev.rename(columns={'item_cnt':'cnt_sum_prev'})\n",
    "\n",
    "train_cat_prev = pd.merge(train_prev, items_df, on=['item_id'], how='left')\n",
    "grp = train_cat_prev.groupby(['date_block_num','shop_id','item_category_id'])\n",
    "train_cat_prev = grp['cnt_sum_prev'].sum().reset_index()\n",
    "train_cat_prev = train_cat_prev.rename(columns={'cnt_sum_prev':'cnt_sum_cat_prev'})\n",
    "train_cat_prev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = np.arange(34)\n",
    "pivT = train_piv[col].T\n",
    "evm_s = pivT.ewm(span=12).mean().T\n",
    "evm_l = pivT.ewm(span=26).mean().T\n",
    "\n",
    "macd = evm_s - evm_l\n",
    "sig = macd.ewm(span=9).mean()\n",
    "\n",
    "train_piv_key = train_piv.loc[:,['shop_id','item_id']]\n",
    "train_evm_list = []\n",
    "\n",
    "for c in col:\n",
    "    sub_evm_s = pd.DataFrame(evm_s.loc[:,c]).rename(columns={c:'cnt_evm_s_prev'})\n",
    "    sub_evm_l = pd.DataFrame(evm_l.loc[:,c]).rename(columns={c:'cnt_evm_l_prev'})\n",
    "    sub_macd = pd.DataFrame(macd.loc[:,c]).rename(columns={c:'cnt_macd_prev'})\n",
    "    sub_sig = pd.DataFrame(sig.loc[:,c]).rename(columns={c:'cnt_sig_prev'})\n",
    "    \n",
    "    sub_evm = pd.concat([train_piv_key, sub_evm_s, sub_evm_l, sub_macd, sub_sig], axis=1)\n",
    "    sub_evm['date_block_num'] = c + 1\n",
    "    train_evm_list.append(sub_evm)\n",
    "    \n",
    "train_evm_prev = pd.concat(train_evm_list)\n",
    "#train_evm_prev.head()\n",
    "train_evm_prev.query(\"shop_id == 2 & item_id == 30\").tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat_monthly.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icats_df['item_category_group'] = icats_df['item_category_name'].apply(lambda x: str(x).split(' ')[0])\n",
    "icats_df['item_category_group'] = pd.Categorical(icats_df['item_category_group']).codes\n",
    "\n",
    "train_cat_group_monthly = pd.merge(train_cat_monthly, icats_df, on=['item_category_id'], how='left')\n",
    "\n",
    "grp = train_cat_group_monthly.groupby(['item_category_group'])\n",
    "train_group = grp.agg({'item_cnt':['mean', 'median', 'std']}).reset_index()\n",
    "train_group.columns = ['item_category_group', 'cnt_mean_group', 'cnt_median_group', 'cnt_std_group']\n",
    "train_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeFeature(source): \n",
    "    d = source\n",
    "    d = pd.merge(d, items_df, on=['item_id'], how='left')\n",
    "    d = pd.merge(d, icats_df, on=['item_category_id'], how='left')\n",
    "    d = pd.merge(d, shops_df, on=['shop_id'], how='left')\n",
    "\n",
    "    d = pd.merge(d, train_price, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_shop_cat, on=['shop_id','item_category_id'], how='left')\n",
    "    d = pd.merge(d, train_city_cat, on=['city_id','item_category_id'], how='left')\n",
    "    d = pd.merge(d, train_group, on=['item_category_group'], how='left')\n",
    "    #d = pd.merge(d, train_last, on=['shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_evm_prev, on=['date_block_num','shop_id','item_id'], how='left')\n",
    "    d = pd.merge(d, train_cat_prev, on=['date_block_num','shop_id','item_category_id'], how='left')\n",
    "\n",
    "    d.drop(['date_block_num', 'shop_id','shop_name','item_id','item_name','item_category_id','item_category_name', 'item_category_group', 'city_id'], axis=1, inplace=True)\n",
    "    d.fillna(0.0, inplace=True)\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_rescale(train, test):\n",
    "    \n",
    "    d = pd.concat([train, test], ignore_index=True)\n",
    "    \n",
    "    d['item_price_inv'] = d['item_price'].values.min()/d['item_price']\n",
    "    d.drop(['item_price'], axis=1, inplace=True)\n",
    "    d['item_price_inv'].fillna(0, inplace=True)\n",
    "    \n",
    "    columns = ['cnt_mean_shop', 'cnt_std_shop', 'cnt_mean_cat_shop', 'cnt_mean_cat_city', 'cnt_mean_group',\n",
    "               'cnt_median_group', 'cnt_std_group', 'cnt_sum_prev', 'cnt_evm_s_prev', 'cnt_evm_l_prev',\n",
    "               'cnt_macd_prev', 'cnt_sig_prev', 'cnt_sum_cat_prev']\n",
    "    \n",
    "    for column in columns:\n",
    "    \n",
    "        d[column] = d[column]/d[column].values.std()\n",
    "    \n",
    "    return d.iloc[:len(train)], d.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mergeFeature(train_monthly)\n",
    "\n",
    "X_train = train_set.drop(['item_cnt'], axis=1)\n",
    "Y_train = train_set[['item_cnt']]\n",
    "\n",
    "X_test = mergeFeature(test_df.drop(['ID'], axis=1))\n",
    "\n",
    "X_train, X_test = num_rescale(X_train, X_test)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train.to_csv(\"X_train.csv.gz\", index=False, compression='gzip')\n",
    "X_test['ID'] = test_df['ID']\n",
    "X_test.to_csv(\"X_test.csv.gz\", index=False, compression='gzip')\n",
    "Y_train.to_csv(\"Y_train.csv.gz\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the X_train, Y_train, X_test are produced, we can work from this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
